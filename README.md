# MoICE: Mixture of In-Context Experts — Репродукция и разбор проблем запуска

Данный репозиторий предназначен для локального воспроизведения экспериментов из работы  
**“Mixture of In-Context Experts Enhance LLMs’ Long Context Awareness” (Lin et al., 2024)**  
с использованием модели TinyLlama и модифицированного пайплайна обучения MoICE.

Помимо базовых инструкций по запуску здесь **подробно документируется, почему эксперимент в текущей конфигурации не был доведен до успешного конца**, и какие подходы были испробованы.

---

## Требования

### Аппаратные

- NVIDIA GPU с поддержкой CUDA (в работе: RTX 4060)
- Драйвер NVIDIA, включенный passthrough в WSL2
- Рекомендуемый объем видеопамяти: ≥ 12 GB

### Программные

- ОС: Linux или WSL2 (в работе: Ubuntu под WSL2)
- Python 3.10
- Conda / Miniconda
- PyTorch с поддержкой CUDA

---

## Установка окружения

Рекомендуемый базовый путь:

```bash
conda create -n moice python=3.10 -y
conda activate moice

# PyTorch с CUDA 11.8
pip install torch==2.1.2+cu118 --index-url https://download.pytorch.org/whl/cu118

# Базовые компоненты для MoICE
pip install transformers==4.33.2
pip install accelerate==0.22.0
pip install deepspeed==0.10.3
pip install sentencepiece protobuf
```

> FlashAttention не обязателен и в WSL2, как правило, не собирается без установленного CUDA Toolkit и `nvcc`.

---

## Скачивание модели TinyLlama

### Через CLI `huggingface-hub`

```bash
pip install huggingface_hub==0.19.3

huggingface-cli download   TinyLlama/TinyLlama-1.1B-Chat-v1.0   --local-dir models/llama2-7b-chat
```

---

## Подготовка датасета

Изначально планировалось использовать **PG-19** (как в оригинальной статье). Из-за проблем с доступом к данным и `git-lfs` под WSL2 был выбран альтернативный набор — **Refined BookCorpus** с Kaggle.

Файл CSV конвертировался в JSON-формат, совместимый с ожиданиями `train-balance.py`:

```bash
python tools/convert_csv_to_json.py   --input path/to/bookcorpus.csv   --train-output data/train_data.json   --eval-output data/eval_data.json
```

---

## Проверка загрузки модели

Перед запуском обучения рекомендуется проверить, что модель и токенизатор корректно загружаются:

```bash
python - << 'EOF'
from transformers import AutoTokenizer, AutoModelForCausalLM

model_path = "models/llama2-7b-chat"

tok = AutoTokenizer.from_pretrained(model_path)
print("tokenizer ok:", tok.__class__.__name__)

model = AutoModelForCausalLM.from_pretrained(model_path)
print("model ok, params:", sum(p.numel() for p in model.parameters())/1e6, "M")
EOF
```

На одном из этапов работы выяснилось, что первоначально скачанный `pytorch_model.bin` был поврежден, что приводило к ошибке:

```text
_pickle.UnpicklingError: Unsupported operand 149
```

После повторной загрузки файла весов проблема была устранена.

---

## Запуск обучения

### Скрипт (оригинальный из репозитория)

```bash
cd training
bash train.sh
```

Этот скрипт запускает DeepSpeed с параметрами (пример):

```bash
deepspeed --num_gpus=4 train-balance.py   --model_name_or_path ../models/llama2-7b-chat   --data_path ../data/train_data.json   --eval_data_path ../data/eval_data.json   ...
```

### Адаптированный-скрипт

Для WSL2 + RTX 4060 был подготовлен упрощенный скрипт `train2.sh`, который вызывает `train-balance.py` напрямую (без DeepSpeed), с теми же аргументами модели и данных, но в режиме одного GPU.

---

## Почему запуск обучения не был доведен до конца

Ниже — детализированный разбор **всех ключевых препятствий**, с которыми столкнулся эксперимент, в хронологическом и логическом порядке.

### 1. Старт на Windows + conda и переход в WSL

Изначально попытка запуска велась в среде Windows (PowerShell) с использованием `conda` / `miniconda`. Уже на этом этапе начали проявляться проблемы:

- нераспознанная команда `conda` до корректной инициализации `conda init`;
- трудности с установкой `bitsandbytes` под Windows (официальная версия ориентирована на Linux);
- необходимость костылей вида `bitsandbytes-windows` и ручного копирования модулей.

В результате было принято решение перенести эксперименты в WSL2 (Ubuntu), где стек PyTorch + CUDA поддерживается более предсказуемо.

---

### 2. Доступ к исходным датасетам (PG-19, Google Cloud, git-lfs, Kaggle)

Была предпринята попытка использовать:

- **PG-19** (DeepMind) через Google Cloud Storage;
- официальный репозиторий PG-19 через `git lfs`.

Возникли проблемы:

- `git lfs` некорректно работал/не был установлен в WSL2;
- скачивание через GCS требовало доп. аутентификации и скриптов.

Далее был сделан переход на Kaggle:

- попытка использовать `kaggle datasets download` наткнулась на `403 Forbidden` (проблемы с API-токеном / правами);
- в итоге датасет был скачан вручную через веб-интерфейс Kaggle (Refined BookCorpus) и затем конвертирован в нужный формат.

Вывод: **данные удалось подготовить**, этот этап, по сути, не стал блокирующим, но потребовал значительного времени.

---

### 3. Версионные конфликты: Transformers ↔ Huggingface Hub ↔ Accelerate

На одном из этапов, после установки/обновления библиотек, возникла критическая несовместимость:

```text
ImportError: huggingface-hub>=0.19.3,<1.0 is required for a normal functioning of this module, but found huggingface-hub==1.1.6.
```

Параллельно:

- `accelerate` требовал новых API из `huggingface_hub`;
- более свежий `accelerate` конфликтовал с версией Transformers, на которую опирается MoICE;
- старые версии `accelerate` не дружили с установленным PyTorch 2.1+.

По сути, требовалась очень точная комбинация версий:
- Transformers примерно 4.33–4.35,
- huggingface-hub < 1.0,
- accelerate в диапазоне ~0.21–0.22,
- PyTorch не ниже 1.13, но не слишком новый.

Подобрать стабильную тройку «Transformers + Accelerate + Huggingface Hub» на готовой системе с уже установленными библиотеками оказалось нетривиально.

---

### 4. BitsAndBytes и PyTorch 2.x

При использовании `bitsandbytes` для 8-битных квантованных весов возникла ошибка:

```text
AttributeError: module 'torch.library' has no attribute 'impl_abstract'
```

Она свидетельствует о том, что установленная версия `bitsandbytes` не совместима с PyTorch 2.1+ (ожидается более старый API).  
После удаления `bitsandbytes` стало ясно, что для целей эксперимента 8-битная квантизация не является критической, и ее можно отключить. Однако сам факт конфликта говорит о хрупкости связки MoICE с современным стэком.

---

### 5. FlashAttention и отсутствие CUDA Toolkit (`nvcc`) в WSL2

Официальный `requirements.txt` MoICE включает:

```text
flash-attn==2.3.2
```

Попытка установки:

```bash
pip install flash-attn==2.3.2
# или
pip install flash-attn==2.5.6 --no-build-isolation
```

заканчивается ошибкой:

```text
OSError: CUDA_HOME environment variable is not set. nvcc was not found.
```

Это означает:

- в WSL2 есть доступ к GPU (через драйверы и CUDA runtime),
- **но нет полноценного CUDA Toolkit** и компилятора `nvcc`,
- flash-attn требует компиляции CUDA-расширений и без этого не устанавливается.

Фактически, для использования FlashAttention в такой конфигурации необходима установка полноценного CUDA Toolkit внутрь WSL2, что значительно усложняет окружение.

---

### 6. Проблемы с `requirements.txt` MoICE

При попытке честно установить все из `requirements.txt`:

```bash
pip install -r requirements.txt
```

возникли несколько проблем:

1. Пакет `dataset==0.1.3` — несуществующая версия (скорее всего, опечатка вместо `datasets` от Hugging Face).
2. Жестко фиксированная версия `flash-attn==2.3.2`, которая не ставится без CUDA Toolkit.
3. Итогом становилась частично установленная среда без ключевых библиотек (`transformers` и др.), так как процесс прерывался на ошибке установки flash-attn.

В результате было принято решение **не полагаться полностью на `requirements.txt`** и собирать окружение вручную, опираясь на реальные версии библиотек.

---

### 7. Интеграция MoICE в LLaMA и ручной патчинг конфигурации

Даже после стабилизации версий и успешной загрузки TinyLlama при использовании модифицированного `modeling_llama.py` для MoICE возникли ошибки:

- отсутствуют поля `base_set`, `topk`, `expert_nums` в `LlamaConfig`;
- дополнительные параметры gate-слоев не инициализируются чекпойнтом.

Это потребовало **ручного патчинга**:

```python
config.base_set = [10000, 17500, 18000, 19000, 20000, 22500, 25000]
config.topk = 7
config.expert_nums = 7
config.output_router_logits = True
config.router_aux_loss_coef = 0.3
```

после чего модель инициализировалась, но уже с предупреждением о том, что новые слои (gates) имеют случайно инициализированные веса и требуют дообучения.

---

### 8. Итоговая точка остановки

На момент последней попытки запуск `train2.sh` упирался в цепочку версионных и бинарных конфликтов:

- необходимость подгонять версии Transformers, Accelerate и Huggingface Hub;
- невозможность установки FlashAttention без CUDA Toolkit;
- сильно перегруженный и частично устаревший `requirements.txt`, нестабильно устанавливающийся в современной системе.

При этом:

- модель инициализируется,
- датасет подготовлен.

**Основной блокирующий фактор** — хрупкость связки «MoICE + старые версии HuggingFace-стека + FlashAttention» в современной среде (PyTorch 2.1, WSL2 без CUDA Toolkit, свежие версии huggingface_hub).

---

### 9. Шардинг весов в компактных моделях

Несмотря на относительную компактность моделей масштаба ~1B параметров (например, TinyLlama-1.1B), их веса, как правило, распространяются в шардированном формате. Вместо одного монолитного файла веса поставляются набором файлов вида:

```
pytorch_model-00001-of-00002.bin
pytorch_model-00002-of-00002.bin
```

или аналогичных файлов в формате `safetensors`. Такая схема используется для:

При попытке интеграции MoICE это создало ряд нетривиальных проблем, напрямую связанных с несовместимостью между шардированными весами и модифицированным механизмом загрузки модели в `transformers`.

Основные проблемы:

1. **Несоответствие ожидаемой структуры весов.** MoICE добавляет в архитектуру дополнительные параметры (gate_i, router_logits, расширенную конфигурацию). Шардированные веса TinyLlama не содержат этих параметров, что приводит к предупреждениям о недоинициализированных слоях и, в ряде случаев, к ошибкам согласования размеров.

2. **Точечные различия в версиях `transformers`.** Старые версии библиотеки некорректно работали с некоторыми форматами шардирования (особенно с весами, сохраненными в смешанном safetensors/bin формате). Новые версии — наоборот — конфликтовали с зависимостями MoICE (особенно accelerate), из-за чего загрузка шардов прерывалась ошибками вроде:
“Unable to load weights … unsupported operand … UnpicklingError …”

3. **Непрозрачность автоматического объединения шардов.**
Хотя HuggingFace Hub автоматически собирает шардированные весы в единый state dict, добавление MoICE-специфичных башен маршрутизации нарушает исходные предположения о структуре модели. В результате возникали ошибки рассинхронизации между количеством слоев, количеством экспертов и ожидаемыми параметрами MoICE.

4. **Ограничения Deepspeed + ZeRO3 при работе с шардами.**
Deepspeed пытается распределить шардированные веса между GPU, но при несовпадении параметров конфигурации или при появлении новых MoICE-специфичных слоев может ошибочно трактовать часть весов как отсутствующую или повреждённую.

Таким образом, использование шардированных весов даже для небольших моделей привело к необходимости:
- пересоздавать окружение под точные версии зависимостей;
- вручную проверять соответствие конфигурации модели исходным весам;
- адаптировать загрузку модели так, чтобы MoICE-конфигурация была добавлена после полного чтения и объединения шардов.

---

## Выводы

1. **MoICE в текущем виде плохо переносим** на произвольные пользовательские окружения; он рассчитан на довольно узкий диапазон версий библиотек и, возможно, специфическую инфраструктуру авторов.
2. Полный запуск обучения требует:
   - либо развертывания Docker/контейнера с зафиксированными версиями,
   - либо строгого воспроизведения окружения авторов (включая CUDA Toolkit).
3. Несмотря на незавершенность запуска, была проделана существенная работа:
   - подготовлен и проверен датасет,
   - загружена и инициализирована модель TinyLlama,
   - интегрированы MoICE-параметры,
   - выявлены и задокументированы все ключевые конфликтные точки.

---

## Связанные материалы

- Оригинальная статья: https://arxiv.org/abs/2405.02765  
- Оригинальный репозиторий MoICE: https://github.com/Exploration-Lab/MoICE  
- TinyLlama модель: https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0

